\documentclass[letterpaper, 11 pt]{article} 
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amsfonts,amssymb,color}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage[dvipsnames]{xcolor}
\definecolor{mygreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{winered}{rgb}{0.8,0,0}
\definecolor{myblue}{rgb}{0,0,0.8}
\usepackage{tikz}
%\usepackage{pmat}
\usepackage{amsthm}
\usepackage{empheq}
\newcommand*\widefbox[1]{\fbox{\hspace{0.2em}#1\hspace{0.2em}}}
\newcommand{\mycaption}[1]{\stepcounter{figure}\raisebox{-7pt}
  {\footnotesize Fig. \thefigure.\hspace{3pt} #1}}
%\theoremstyle{plain}
\newtheorem{problem}{Problem}
\newtheorem{condition}{Condition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\mc}{\mathcal}
\newcommand{\mb}{\mathbf}
%\newtheorem{proof}{Proof}
\newtheorem{example}{Example}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}
%\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{epsfig}
\usepackage{array}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{tikz}
\usepackage{relsize}
\usetikzlibrary{shapes,arrows}
\usepackage[hidelinks]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor={winered},
    citecolor={mygreen}
}
%\usepackage{subcaption}
\usepackage{cite}
%\theoremstyle{plain}
%\usepackage{scalerel}
%\DeclareMathOperator*{\Bigcdots}{\scalerel*{\cdots}{\bigodot}}
\usepackage[margin=1in]{geometry}
\title{\LARGE Adagrad, Adam, and AMSGrad}
\author{Beilong Tang
% <-this % stops a space
\thanks{Beilong Tang is with the Department of ECE at North Carolina State University. Email: {\tt btang5@ncsu.edu}.}}
\date{}
\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}
\begin{abstract}
In this project, I will analyze the convergence of three famous adaptive methods: Adagrad, Adam, and AMSGrad in a simple manner. We limit our analysis for convex functions.
\end{abstract} 

\section{Introduction}

\subsection{Related work}
In class, we studied the convergence proofs for Stochastic Gradient Descent (SGD) and Gradient Descent (GD). These proofs typically rely on selecting step sizes using properties such as smoothness or strong convexity of the loss function. However, in most deep learning applications, the loss landscape is highly non-convex and often nonsmooth, making such assumptions invalid. This raises a natural question: how can we design SGD-like algorithms that guarantee convergence without requiring carefully tuned step sizes?

A major step toward answering this question was the introduction of AdaGrad \cite{duchi2011adagrad}, one of the first adaptive optimization algorithms. AdaGrad adjusts the step size dynamically using the cumulative history of squared gradients and achieves strong theoretical convergence guarantees. Despite this, its practical performance is often suboptimal: the effective learning rate continues to decay over time, and early gradients may dominate the accumulated statistics, causing the algorithm to slow down excessively.

To address these issues, Adam \cite{Kingma2014adam} was proposed. Adam uses exponential moving averages of both first- and second-order moments to compute adaptive step sizes. It retains the theoretical flavor of AdaGrad while performing significantly better in large-scale deep learning tasks, eventually becoming one of the most widely used optimizers. However, the original convergence proof of Adam contains several issues, later identified and corrected in \cite{reddi2019convergence}, which introduced AMSGrad, a variant of Adam that fixes these shortcomings.

\subsection{Outline}
In this project, I will begin by presenting a simplified convergence proof of AdaGrad and analyzing its behavior in both sparse and dense gradient regimes. I will then discuss the practical limitations of AdaGrad and motivate the development of Adam. Next, I will outline a simplified version of Adamâ€™s convergence proof and highlight the mistakes in the original analysis. Finally, I will introduce AMSGrad and explain how it resolves these issues, leading to a more robust variant of Adam.

\section{Problem Formulation}

Similar to the SGD setting in class, our loss function is noisy. We denote the convex noisy loss objective as $f(x)$ where $x$ denotes our model parameters. Our goal is to minimize $\mathbb{E}[f(x)]$ w.r.t $x$. We observe $f_1(x_1), f_2(x_2), f_3(x_3),\dots,f_T(x_T)$ as our loss functions at time steps $1,2,\dots,T$. One reason of the stochasticity comes from the training of minibatches (same to SGD 2 as we discussed in class). We denote $g_t=\nabla_{x_t} f(x_t)$ be the gradient at the $t$th step. Let $g_{i:t}=[g_1, g_2,\dots,g_t]$ detnoes the matrix obtained by the contatenating the gradients until time $t$. We use $g_t^2$ to denote the element-wise square of vector $g_t$.

Instead of knowing the true loss function, we can only observe noisy loss functions until our current time step $T$. Hence, we want to minimize our regret:
\begin{equation}
    R(T)=\sum_{t=1}^T[f_t(x_t)-f_t(x^*)] 
\end{equation}
where $x^{*}=\operatorname{arg}\operatorname*{min}_{x\in\mathcal{X}}\sum_{t=1}^{T}f_{t}(x)$ denotes our optimal solution, and $\mathcal{X}$ denotes our parameter domain. We can view $R(T)$ as the average of loss until the current loss. We use it because we cannot observe our true loss due to the noise. Hence, using single loss will lead to inaccurate estimation, thus we take the average.  

\section{Main Results/Algorithms}

\subsection{Adagrad}
\begin{algorithm}[t]
\caption{Adagrad Algorithm}
\label{alg:adagrad}
\begin{algorithmic}[1] % or without [1] if disable the line number.
\Require $(\text{Step size}) \eta>0$, $x_1 = 0$
\For{$t = 1$ {\bf to} $T$}
    \State $g_t = \nabla f_t(x_t)$
    \State $H_t = \text{diag}(\sum_{i=1}^{T}g_i^2)$
    \State $x_{t+1} = x_t - \eta H_t^{-1}g_t \quad \textit{(Update Rule)}$
\EndFor
\end{algorithmic}
\end{algorithm}

The simplified version of Adagrad is given in Algorithm~\ref{alg:adagrad}. The main update rule is line 4 where we use the summation of the past squared gradients to adjust the learning rate.

Next, we give a simplified proof of the convergence of Adagrad.

Following Adagrad \cite{duchi2011adagrad}, we let
\begin{equation}
    B_{\psi}(x,y) = \psi(x) - \psi(y) - \langle \nabla \psi(y), x-y\rangle
\end{equation} denote the Bregman divergence where $\psi$ is a strongly convex and differentiable function.

One specific update rule is called the composite mirror descent update rule. It is also mentioned in Adagrad;
\begin{equation} \label{eq:adagrad_composite}
    x_{t+1}=\operatorname{arg}\operatorname*{min}_{x}\{\eta \langle  g_t,x\rangle + B_{\psi_t}(x,x_t) \}
\end{equation} which is the Equation (4) in Adagrad \cite{duchi2011adagrad} where the fixed regualization function and the projection is omitted for simplicity. We let $\psi_t(x) = \frac{1}{2}x^TH_t x$. It should be obvious that $\psi_t(x)$ is strongly convex because $H_t \succeq 0$.

The outline of the proof is that I will firstly prove that the update rule in Adagrad's algorithm is the same as the update rule in Equation~\ref{eq:adagrad_composite}. Then I will use a property of the update rule in Equation~\ref{eq:adagrad_composite} to prove the convergence.

\begin{theorem}
The update rule in Adagrad is equivalent to the update rule in Equation~\ref{eq:adagrad_composite}.
\end{theorem}

\begin{proof}
Let $h_t(x) =\eta \langle  g_t,x\rangle + B_{\psi_t}(x,x_t)$, and let  \begin{equation}
% h(x) = \eta \langle  g_t,x\rangle + B_{\psi_t}(x,x_t)
x_{t+1} = x_t - \eta H^{-1}_t g_t
\end{equation}
Then 
\begin{equation} \label{eq:thm1}
    \eta g_t + H_t x_{t+1} - H_t x_t = 0
\end{equation}
Also, we know that 
$$
\nabla_{x_{t+1}} h_t(x_{t+1}) = \eta g_t + H_t x_{t+1} - H_t x_t
$$
If we set $\nabla_x h_t(x)=0$, we will get exactly Equation~\ref{eq:thm1}. Also, we know that $\operatorname{arg}\operatorname{min}_x h_t(x)$ is equal to finding the point where $\nabla_x h_t(x)=0$. Hence, we conclude our proof.
\end{proof}

Next, we will provide one proposition and use it to prove the convergence.

\begin{proposition}
    Let sequence $\{x_t\}$ be defined by the update rule in Equation~\ref{eq:adagrad_composite}. Then for any $x^*$:
    \begin{align}
& \sum_{t=1}^{T} \bigl(f_t(x_t) - f_t(x^*)\bigr) \\
&\le \frac{1}{\eta} B_{\psi_1}(x^*,x_1)
 + \frac{1}{\eta}\sum_{t=1}^{T-1}\!\left[B_{\psi_{t+1}}(x^*,x_{t+1}) - B_{\psi_t}(x^*,x_{t+1})\right] \nonumber
+\frac{\eta}{2}\sum_{t=1}^T \left\| g_t \right\|_{\psi_t^*}^2.
\end{align}

where $\psi_t^*$ is the assosiated 
    
\end{proposition}

\section{Conclusion}
A short conclusion summarizing your findings in the course project, and a discussion of open directions. Limit to at most a page.  





\bibliographystyle{unsrt}
%\setcitestyle{authoryear,open={square} and close={square}}
\bibliography{refs}

% \begin{algorithm}[t]
% \caption{Generic Adaptive Method Setup}
% \label{alg:generic_adaptive}
% \begin{algorithmic}[1] % or without [1] if disable the line number.
% \Require $x_1 \in \mathcal{F}$, step size $\{\alpha_t > 0\}_{t=1}^T$, sequence of functions $\{\phi_t, \psi_t\}_{t=1}^T$
% \For{$t = 1$ {\bf to} $T$}
%     \State $g_t = \nabla f_t(x_t)$
%     \State $m_t = \phi_t(g_1, \ldots, g_t)$ \textbf{and} $V_t = \psi_t(g_1, \ldots, g_t)$
%     \State $\hat{x}_{t+1} = x_t - \alpha_t m_t / \sqrt{V_t}$
%     \State $x_{t+1} = \Pi_{\mathcal{F}, \sqrt{V_t}}(\hat{x}_{t+1})$
% \EndFor
% \end{algorithmic}
% \end{algorithm}

% \begin{theorem}[Convergence of Example Algorithm]
% Let $f:\mathbb{R}^d \to \mathbb{R}$ be a convex function. Suppose the sequence $\{x_t\}$ is generated by the update rule
% \[
% x_{t+1} = x_t - \eta \nabla f(x_t),
% \]
% with a constant step size $\eta > 0$. Then the sequence satisfies
% \[
% f(x_T) - f(x^\ast) \le O\left( \frac{1}{T} \right),
% \]
% where $x^\ast$ is a minimizer of $f$.
% \end{theorem}

% \begin{proof}
% By convexity of $f$, we have
% \[
% f(x_t) - f(x^\ast) \le \langle \nabla f(x_t), x_t - x^\ast \rangle.
% \]
% Using the update rule, expand the squared distance:
% \[
% \| x_{t+1} - x^\ast \|^2
% = \| x_t - \eta \nabla f(x_t) - x^\ast \|^2.
% \]
% Rearranging gives
% \[
% \langle \nabla f(x_t), x_t - x^\ast \rangle 
% \le \frac{ \|x_t - x^\ast\|^2 - \|x_{t+1} - x^\ast\|^2 }{2\eta}
% + \frac{\eta}{2}\|\nabla f(x_t)\|^2.
% \]
% Summing over $t = 1,\dots,T$ telescopes the distances and yields the claimed result.
% \end{proof}


\end{document}
